#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
K-Nearest Neighbor classifier in Stata15
\end_layout

\begin_layout Author
Nanjin Zeng(id:15220162202482) WISE IUEC 2016
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
In the lecture Classification, we have learnt a non-parametric method, K-nearest
 neighborclassifer.
 This method is founded by Cover and Hart (1967).
 As its founders stated, it may be the simplest nonparametric decision procedure
\begin_inset CommandInset citation
LatexCommand cite
key "key-1"
literal "false"

\end_inset

, using the idea of 
\begin_inset Quotes eld
\end_inset

majority vote
\begin_inset Quotes erd
\end_inset

.
 For the simplified version we learnt in class, it can be thought as
\begin_inset CommandInset citation
LatexCommand cite
key "key-2"
literal "false"

\end_inset


\end_layout

\begin_layout Quote
\begin_inset Formula $\hat{p}\left(y=j|x\right)=\frac{1}{K}\sum_{i\in\mathcal{N_{K}\left(\textrm{x}\right)}}\mathcal{\textrm{I}\textrm{\left(y_{i}=j\right)}}$
\end_inset

with the desicion boundary of 0.5, where 
\begin_inset Formula $y\in\left\{ 1,\ldots,j\right\} $
\end_inset


\end_layout

\begin_layout Standard
Though this method has some drawbacks, for example, the strong influence
 of a more frequent class, it is still a method being widely used in machine
 learning.
 In this blog, I would talk about how to perform KNN classification in Stata15
 using the Credit Card Default data as same as our teacher demonstrated
 in R.
\end_layout

\begin_layout Section
Syntax
\end_layout

\begin_layout Quotation
discrim knn 
\bar under
varlist
\bar default
 [if] [in] [weight], group(groupvar) k(#) [options]
\end_layout

\begin_layout Standard
The groupvar is the variable of categories we want to classfied.
 Notice that it should be a numerical variable.
 If your categories are defined in string like the brand of ketchup, you
 should pre-treat them.
 For the k(#), it is the number of nearest neighbors you want to take in
 consideration.
 Large k reduced the noise on the classiifcation, but the boundaries would
 be less distinct.
 So, there is a trade-off setting the k.
 Some method could help you determin it wisely.
\begin_inset CommandInset citation
LatexCommand cite
key "key-3"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
The command performing KNN in stata is not some complex, but it contains
 some interesting options.
 In this part, I would introduce them and give a brief introduction to the
 idea behind them.
\end_layout

\begin_layout Subsection
measure
\end_layout

\begin_layout Standard
For the measure of similarity or dissimilarity, we should compute the "distance"
 from the neighbour to the point being testing.
 Then we can choose the closest neighbors and make a "vote".
 Stata gives us lots of measure in distance for us to choose.
\begin_inset CommandInset citation
LatexCommand cite
key "key-7"
literal "false"

\end_inset


\end_layout

\begin_layout Quote
measure(L2) / measure(Euclidean)
\end_layout

\begin_layout Standard
This is the default one.
 It means that we use Euclidean distance.
\end_layout

\begin_layout Standard
\begin_inset Formula $E(x,y)=\sqrt{\sum_{i=0}^{n}\left(x_{i}-y_{i}\right)^{2}}$
\end_inset


\end_layout

\begin_layout Quote
measure(L1) / measure(manhattan)
\end_layout

\begin_layout Standard
The name comes from the distance taking taxi between blocks in manhattan.
 It means that we sum the absolute-value distance in each dimension.
\end_layout

\begin_layout Standard

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula $E(x,y)=\sum_{i=0}^{n}|x_{i}-y_{i}|$
\end_inset


\end_layout

\begin_layout Standard
Many other measures could be used, based on the actual need, including Minkowski
 distance, Canberra distance and so on.
 Noticed that if you choose a continuous measure, you should normalize your
 data.
 That would make different variable unit-free and comparable.
\end_layout

\begin_layout Subsection
priors
\end_layout

\begin_layout Standard
In a bayesian's view, if we know the prior probabilities in each categories,
 we should take it into consideration.
 This term allow us to specify the prior probabilities.
 Moreover, it could ease the problem of majority vote making the frequent
 class dominate the prediction.
\end_layout

\begin_layout Quote
priors(proportional)
\end_layout

\begin_layout Standard
It means that we do not make any adjustement.
 The prediction is simply based on observed data.
\end_layout

\begin_layout Quote
priors(equal)
\end_layout

\begin_layout Standard
This is the default one.
 It means that we specify a non-informative prior and assume an equal probabilit
y in each group.
\end_layout

\begin_layout Quote
priors(matname)
\end_layout

\begin_layout Standard
You can use a matrix to specify the prior probalities for each categories.
\end_layout

\begin_layout Subsection
ties
\end_layout

\begin_layout Standard
Ties refer to the multiple equal outcomes
\begin_inset CommandInset citation
LatexCommand cite
key "key-4"
literal "false"

\end_inset

 when we doing prediction using the trainning data.
 For example, when 
\begin_inset Formula $k=2$
\end_inset

 if the nearest 2 point belongs to different groups, we cannot determin
 the prediction for this point using the rule of majority-vote.
 Though we can reduce the oucurrence by some method, like the simplest "k
 should be odd number", it would bother us we we do KNN in a high dimension.
 In the annotation(17) on the slides, ties are broken at random.
 It means these points would be randomly assigned to each category.
 (in stata, "ties(random)")Also we could define how to treat them in stata.
\end_layout

\begin_layout Quote
ties(missing)
\end_layout

\begin_layout Standard
This is the default one.
 It means we do not make any predictions in these point.
\end_layout

\begin_layout Quote
ties(first)
\end_layout

\begin_layout Standard
It means that ties are set to the first tied group.
\end_layout

\begin_layout Quote
ties(nearest)
\end_layout

\begin_layout Standard
It means that the predition in ties would base on the nearest point.
 But it may still remain in a tie in some extreme situations.
 Then we do not make any predictions in these point.
\end_layout

\begin_layout Section
Method Demonstration
\end_layout

\begin_layout Standard
To show it intuitively, I use the same data (Credit Card Default data) in
 Stata15.
 In this example, as we did in the lecture using R, we perform the KNN on
 the trainning data and see the effect 
\begin_inset Formula $\left(E_{in}\right)$
\end_inset

.
\end_layout

\begin_layout Quote
import delimited "C:
\backslash
Users
\backslash
91367
\backslash
Desktop
\backslash
junior 2nd term
\backslash
microeconometrics
\backslash
hw
\backslash
knn
\backslash
Default.csv"
\end_layout

\begin_layout Quote
generate default_num=0 if default=="No"
\end_layout

\begin_layout Quote
replace default_num=1 if default=="Yes"
\end_layout

\begin_layout Standard
To be recognize as a group variable, since categories are defined in string
 "No" and "Yes", we should give them a number as an indicator.
\end_layout

\begin_layout Quote
egen ba_std=std(balance)
\end_layout

\begin_layout Quote
egen in_std=std(income)
\end_layout

\begin_layout Standard
We normalize the variable to have a zero mean and 1 stand error.
 That is a necessary step using KNN, otherwise you put unequal weight in
 the variables and the distances are unreliable.
\end_layout

\begin_layout Quote
discrim knn ba_std in_std, group(default_num) k(5) priors(proportional)
 ties(random)
\end_layout

\begin_layout Standard
The result using KNN
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename knn.jpg

\end_inset


\end_layout

\begin_layout Standard
From this table, it reveals that the error is not so small.
 We make 205 false-negative prediction and 50 false-positive predictions
 on the trainning data.
 In this case, false-negative result is critical because it means that we
 give credit to someone who finally defaulted.
 However, this result is on the trainning data.
 We should combine this idea with other method like cross-validation to
 evaluate the validity of our classifier.
\end_layout

\begin_layout Section
Some notices using K-Nearest Neighbor classifier
\end_layout

\begin_layout Standard
From the example, we can feel some drawbacks in our "simple" K-nearest Neighbor
 classifier we need to notice.
\end_layout

\begin_layout Enumerate
When the program runs into the final part of KNN, it takes a long time until
 it gives out the result.
 That is something being talked in almost every materials about KNN.
 As a "lazy method", it retain the trainning data and search through the
 data for the k nearest observations each time a classificatuion or prediction
 is performed.
\begin_inset CommandInset citation
LatexCommand cite
key "key-6"
literal "false"

\end_inset

 However, you could ingore the trainning data after you build the classifier
 performing linear discriminant analysis or logistic discriminant analysis.
 So, if you have a large number of sample, you should consider if it is
 proper to use K-Nearest Neighbor Classifier.
\end_layout

\begin_layout Enumerate
As I mention serveal times above, the KNN suffered from the drawback of
 majority vote.
 One measure is to put the prior probabilities into consideration.
 (using prior(matname)) But a simple one is to change our method to "weighted
 k-nearest neighbor classifier"
\begin_inset CommandInset citation
LatexCommand cite
key "key-5"
literal "false"

\end_inset

 A common way to weight is to giving each neighbor a weight of 
\begin_inset Formula $\frac{1}{d}$
\end_inset

, where 
\begin_inset Formula $d$
\end_inset

 is the distance to the neighbor.
 It reduces the influence from a further point, but not so much comparing
 to setting k=1.
 That may work well when the difference in the number of each categories
 is large.
 It prevents the result being dominated by a more frequent group.
\end_layout

\begin_layout Enumerate
Though K-Nearest neighbor is a non-parametric method, it still have assumption.
 The basic idea is the result should be locally homogeneous.
 Using a intuitive example talking by many others "Your grandparents are
 republicants.
 Your parents are republicants.
 Your neighbour are all repubilicants.
 But you can still be a democrats!" You should check your economic idea
 if it is reasonable to imply this "lazy method".
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-1"
literal "false"

\end_inset

T.
 M.
 Cover, P.
 E.
 Hart.
 "Nearest Neighbor Pattern Classification"[J].
 IEEE Trans.
 on Information Theory, Vol.
 13, No.
 1.
 (January 1967), pp.
 21-27
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-2"

\end_inset

Jiaming Mao.
 "Classification"[Z].2019-04-10.personal copy
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-3"

\end_inset

Everitt, B.
 S., Landau, S., Leese, M.
 and Stahl, D..
 "Miscellaneous Clustering Methods, in Cluster Analysis"[M[.John Wiley &
 Sons, Ltd, Chichester, UK.
 2011
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-7"

\end_inset

Stata Statistics/Data Analysis.
 "measure_option"[Z].
 2019-04-11.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-4"
literal "false"

\end_inset

Joseph M.Hilbe.
 "Logistic Regression Models"[M].Chapman and Hall/CRC.
 2009
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-6"

\end_inset

Stata Statistics/Data Analysis.
 "discrim knn"[Z].
 2019-04-11.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-5"
literal "false"

\end_inset

S.
 A.
 Dudani.
 The Distance-Weighted k-Nearest Neighbor Rule"[J].
 IEEE Trans.
 on Systems, Man, and Cybernetics, Vol.
 SMC-6 (1976), pp.
 325-327
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-11"

\end_inset

Charles D Mallah, James Orwell.
 "Probabilistic Classication from a K-Nearest-Neighbour Classier".Computational
 Research 1(1): 1-9, 2013
\end_layout

\begin_layout Section
Weekly Recommended Reading by myself
\end_layout

\begin_layout Quotation
The policy of eliminating tolls during holidays generates a significant
 loss of social welfare.
\end_layout

\begin_layout Standard
JOURNAL ARTICLE
\end_layout

\begin_layout Standard
Highway toll and air pollution: Evidence from Chinese cities
\end_layout

\begin_layout Standard
Shihe Fu, Yizhen Gu
\end_layout

\begin_layout Standard
Journal of Environmental Economics and Management Volume 83, May 2017, Pages
 32-49
\end_layout

\begin_layout Standard
DOI: 10.1016/j.jeem.2016.11.007
\end_layout

\begin_layout Standard
This policy is well known and supported by a large number citizens.
 However, whether this policy is economical enough to imply is still being
 discussed.
 For example, most of us has experienced that the congestion in highway
 is unavoidable in national holiday.
 This paper give us an reasonable estimation on the welfare loss due to
 the policy.
 Moreover, it analyzes that the drivers do not shifted their trips from
 before and after “no toll” periods to the “no toll” period.
 That makes the result more reliable.
\end_layout

\begin_layout Section
link
\end_layout

\begin_layout Standard
This article has been published on my own blog.
 Click to visit.
 https://nanjinzeng.github.io/2019/04/11/K-Nearest-Neighbor-Classifier-in-Stata15/
\end_layout

\end_body
\end_document
